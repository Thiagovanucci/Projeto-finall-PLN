{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v-nAZl2Y-Zk8"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC0524OgIs-l",
        "outputId": "d40f6aeb-be90-4545-893a-daf19ed383c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 80s 162ms/step - loss: 0.2117 - accuracy: 0.9625 - val_loss: 0.0995 - val_accuracy: 0.9738\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 66s 140ms/step - loss: 0.0544 - accuracy: 0.9834 - val_loss: 0.0797 - val_accuracy: 0.9772\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 65s 138ms/step - loss: 0.0330 - accuracy: 0.9903 - val_loss: 0.0820 - val_accuracy: 0.9813\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.0183 - accuracy: 0.9955 - val_loss: 0.0866 - val_accuracy: 0.9838\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 63s 135ms/step - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.0945 - val_accuracy: 0.9843\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 63s 135ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.1039 - val_accuracy: 0.9837\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 64s 137ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.1103 - val_accuracy: 0.9835\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.1136 - val_accuracy: 0.9836\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1169 - val_accuracy: 0.9835\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 64s 136ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1239 - val_accuracy: 0.9830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3d0f1859f0>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "#  recurso necessários do NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_conll_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    frases = []\n",
        "    tags = []\n",
        "    frase_atual = []\n",
        "    tags_atual = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip() == '':\n",
        "            if frase_atual and tags_atual:\n",
        "                frases.append(frase_atual)\n",
        "                tags.append(tags_atual)\n",
        "                frase_atual = []\n",
        "                tags_atual = []\n",
        "        else:\n",
        "            palavra, _, _, tag = line.strip().split()\n",
        "            frase_atual.append(palavra)\n",
        "            tags_atual.append(tag)\n",
        "\n",
        "    return frases, tags\n",
        "\n",
        "# dados de treinamento e teste\n",
        "frases_train, tags_train = read_conll_data('/content/train.txt')\n",
        "frases_test, tags_test = read_conll_data('/content/test.txt')\n",
        "\n",
        "#  vocabulário de palavras e tags\n",
        "vocabulario_palavras = set(word for frase in frases_train for word in frase)\n",
        "vocabulario_tags = set(tag for tags_frase in tags_train for tag in tags_frase)\n",
        "\n",
        "#  dicionários para mapear palavras e tags para índices\n",
        "palavra_para_indice = {palavra: i + 1 for i, palavra in enumerate(vocabulario_palavras)}\n",
        "tag_para_indice = {tag: i for i, tag in enumerate(vocabulario_tags)}\n",
        "\n",
        "# Função para converter frases e tags em sequências de índices\n",
        "def process_data(frases, tags, palavra_para_indice, tag_para_indice, max_len):\n",
        "    frases_indices = [[palavra_para_indice.get(palavra, 0) for palavra in frase] for frase in frases]\n",
        "    tags_indices = [[tag_para_indice[tag] for tag in tags_frase] for tags_frase in tags]\n",
        "\n",
        "    frases_preenchidas = pad_sequences(frases_indices, maxlen=max_len, padding='post')\n",
        "    tags_preenchidas = pad_sequences(tags_indices, maxlen=max_len, padding='post')\n",
        "\n",
        "    tags_categoricas = [to_categorical(tags_frase, num_classes=len(vocabulario_tags)) for tags_frase in tags_preenchidas]\n",
        "\n",
        "    return frases_preenchidas, np.array(tags_categoricas)\n",
        "\n",
        "# Encontra o comprimento máximo das sequências em ambos os conjuntos de dados\n",
        "max_len = max(max(len(frase) for frase in frases_train), max(len(frase) for frase in frases_test))\n",
        "\n",
        "# Pré-processamento os dados de treinamento e teste\n",
        "X_train, y_train = process_data(frases_train, tags_train, palavra_para_indice, tag_para_indice, max_len)\n",
        "X_test, y_test = process_data(frases_test, tags_test, palavra_para_indice, tag_para_indice, max_len)\n",
        "\n",
        "#  modelo de NER\n",
        "modelo = Sequential()\n",
        "modelo.add(Embedding(input_dim=len(vocabulario_palavras) + 1, output_dim=32, input_length=max_len))\n",
        "modelo.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
        "modelo.add(TimeDistributed(Dense(len(vocabulario_tags), activation='softmax')))\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "modelo.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extrair_entidades(texto, modelo, palavra_para_indice, indice_para_tag, max_len):\n",
        "\n",
        "    palavras = word_tokenize(texto)\n",
        "\n",
        "    # Converte as palavras em índices\n",
        "    palavra_indices = [palavra_para_indice.get(palavra, 0) for palavra in palavras]\n",
        "\n",
        "    # Preenche a sequência de índices\n",
        "    palavra_indices_preenchidas = pad_sequences([palavra_indices], maxlen=max_len, padding='post')\n",
        "\n",
        "    # Usao modelo para prever as tags\n",
        "    tags_predicao = modelo.predict(palavra_indices_preenchidas)\n",
        "\n",
        "    # Converte as tags previstas em strings\n",
        "    tags_predicao_strings = [indice_para_tag[np.argmax(tag)] for tag in tags_predicao[0][:len(palavras)]]\n",
        "\n",
        "    # Extrai as entidades nomeadas\n",
        "    entidades = []\n",
        "    entidade_atual = []\n",
        "    tag_atual = None\n",
        "\n",
        "    for palavra, tag in zip(palavras, tags_predicao_strings):\n",
        "        if tag.startswith('B-'):\n",
        "            if entidade_atual:\n",
        "                entidades.append((' '.join(entidade_atual), tag_atual))\n",
        "                entidade_atual = []\n",
        "            tag_atual = tag[2:]\n",
        "            entidade_atual.append(palavra)\n",
        "        elif tag.startswith('I-') and tag[2:] == tag_atual:\n",
        "            entidade_atual.append(palavra)\n",
        "        else:\n",
        "            if entidade_atual:\n",
        "                entidades.append((' '.join(entidade_atual), tag_atual))\n",
        "                entidade_atual = []\n",
        "                tag_atual = None\n",
        "\n",
        "    if entidade_atual:\n",
        "        entidades.append((' '.join(entidade_atual), tag_atual))\n",
        "\n",
        "    return entidades\n",
        "\n",
        "# Cria um dicionário para mapear índices para tags\n",
        "indice_para_tag = {i: tag for tag, i in tag_para_indice.items()}\n",
        "\n",
        "\n",
        "texto = \"\"\"Last week, Alice Johnson, a software engineer from San Francisco, attended the annual Tech Innovators Conference held at the Moscone Center.\n",
        " During the event, she met John Smith, the CEO of FutureTech Inc., a leading company in artificial intelligence and robotics.\n",
        "  They discussed the latest advancements in machine learning and the potential applications of AI in various industries, such as healthcare and finance.\n",
        "   Alice was particularly interested in the presentation given by Dr. Jane Doe from Stanford University, who shared her research on natural language processing and its implications for the future of human-computer interaction.\"\"\"\n",
        "\n",
        "palavras = word_tokenize(texto)\n",
        "\n",
        "\n",
        "\n",
        "entidades = extrair_entidades(texto, modelo, palavra_para_indice, indice_para_tag, max_len)\n",
        "\n",
        "print(\"Entidades extraídas:\", entidades)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUP_OD4HIJjy",
        "outputId": "65ee436d-c627-43d7-eaec-bb078bbdc615"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 49ms/step\n",
            "Entidades extraídas: [('Alice Johnson', 'PER'), ('San Francisco', 'LOC'), ('Tech Innovators Conference held', 'ORG'), ('John Smith', 'PER'), ('robotics', 'ORG'), ('AI', 'ORG'), ('presentation given', 'ORG'), ('Jane', 'PER'), ('implications for the', 'ORG')]\n"
          ]
        }
      ]
    }
  ]
}